# import torch
# import torch.nn as nn
# from torch.utils.data import DataLoader, TensorDataset
# from model import Transformer
# import torch.optim as optim
# import requests
# import os
# import matplotlib.pyplot as plt
# import numpy as np
# import random

# # ==================== éšæœºç§å­è®¾ç½® ====================
# def set_seed(seed=42):
#     torch.manual_seed(seed)
#     if torch.cuda.is_available():
#         torch.cuda.manual_seed(seed)
#         torch.cuda.manual_seed_all(seed)
#     np.random.seed(seed)
#     random.seed(seed)
#     # ç¡®ä¿ç¡®å®šæ€§è®¡ç®—
#     torch.backends.cudnn.deterministic = True
#     torch.backends.cudnn.benchmark = False
#     print(f"éšæœºç§å­è®¾ç½®ä¸º: {seed}")

# # è®¾ç½®éšæœºç§å­
# set_seed(42)

# # --------------------------
# # 1. è¶…å‚æ•°è®¾ç½®
# # --------------------------
# seq_len = 128
# batch_size = 32
# n_epochs = 50
# lr = 1e-5
# dropout = 0.1
# d_model = 128
# num_heads = 8
# num_layers = 4
# d_ff = 512
# pad_idx = 0

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# file_path = "Transformer/data/tiny_shakespeare.txt"  

# if os.path.exists(file_path):
#     with open(file_path, 'r', encoding='utf-8') as f:
#         text = f.read()
#     print(f"ä»æœ¬åœ°æ–‡ä»¶è¯»å–æˆåŠŸï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")
# else:
#     print(f"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
#     exit(1)

# # åˆ†å‰²æ•°æ®é›†
# split_idx = int(0.9 * len(text))
# train_text = text[:split_idx]
# val_text = text[split_idx:]

# print(f"è®­ç»ƒé›†é•¿åº¦: {len(train_text)} å­—ç¬¦")
# print(f"éªŒè¯é›†é•¿åº¦: {len(val_text)} å­—ç¬¦")

# # åˆ›å»ºå•è¯æ˜ å°„
# words = text.split()
# unique_words = sorted(list(set(words)))
# vocab_size = len(unique_words)
# print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size}")

# # åˆ›å»ºå•è¯åˆ°ç´¢å¼•çš„æ˜ å°„å’Œç´¢å¼•åˆ°å•è¯çš„æ˜ å°„
# word_to_idx = {word: idx for idx, word in enumerate(unique_words)}
# idx_to_word = {idx: word for idx, word in enumerate(unique_words)}
# # ä½¿ç”¨æ­£ç¡®çš„å˜é‡å
# stoi = word_to_idx  
# itos = idx_to_word  

# def create_sequences(text, seq_len, stoi):
#     """åˆ›å»ºè®­ç»ƒåºåˆ—"""
#     words = text.split()
#     data = torch.tensor([word_to_idx.get(word, 0) for word in words], dtype=torch.long)

#     # åˆ›å»ºåºåˆ—
#     num_sequences = len(data) // seq_len
#     total_length = num_sequences * seq_len
#     data = data[:total_length]
    
#     # é‡å¡‘ä¸ºåºåˆ—
#     sequences = data.view(num_sequences, seq_len)
    
#     # è¾“å…¥å’Œç›®æ ‡ (teacher forcing)
#     inputs = sequences[:, :-1]
#     targets = sequences[:, 1:]
    
#     return inputs, targets

# print("=== å¤„ç†è®­ç»ƒæ•°æ® ===")
# train_inputs, train_targets = create_sequences(train_text, seq_len, stoi)
# val_inputs, val_targets = create_sequences(val_text, seq_len, stoi)

# print(f"è®­ç»ƒé›†ï¼š{train_inputs.shape} | éªŒè¯é›†ï¼š{val_inputs.shape}")

# # åˆ›å»ºDataLoaderï¼ˆåŒ…å«éšæœºç§å­è®¾ç½®ï¼‰
# def seed_worker(worker_id):
#     worker_seed = torch.initial_seed() % 2**32
#     np.random.seed(worker_seed)
#     random.seed(worker_seed)

# g = torch.Generator()
# g.manual_seed(42)

# train_data = TensorDataset(train_inputs, train_targets)
# val_data = TensorDataset(val_inputs, val_targets)
# train_loader = DataLoader(
#     train_data, 
#     batch_size=batch_size, 
#     shuffle=True, 
#     drop_last=True,
#     worker_init_fn=seed_worker,
#     generator=g
# )
# val_loader = DataLoader(
#     val_data, 
#     batch_size=batch_size, 
#     shuffle=False, 
#     drop_last=False
# )

# # --------------------------
# # æ¨¡å‹åˆå§‹åŒ–ä¸ç¨³å®šæ€§æ”¹è¿›
# # --------------------------
# model = Transformer(
#     src_vocab_size=vocab_size,
#     tgt_vocab_size=vocab_size,
#     d_model=d_model,
#     num_heads=num_heads,
#     num_layers=num_layers,
#     d_ff=d_ff,
#     dropout=dropout,
#     pad_idx=pad_idx
# ).to(device)

# # è‡ªå®šä¹‰æƒé‡åˆå§‹åŒ– - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
# def init_weights(m):
#     if isinstance(m, nn.Linear):
#         nn.init.xavier_uniform_(m.weight, gain=0.1)
#         if m.bias is not None:
#             nn.init.constant_(m.bias, 0)
#     elif isinstance(m, nn.Embedding):
#         nn.init.normal_(m.weight, mean=0, std=0.01)
# model.apply(init_weights)

# # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
# criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)
# optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2, betas=(0.9, 0.98), eps=1e-9)

# # å­¦ä¹ ç‡è°ƒåº¦å™¨
# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)

# # å‚æ•°è®¡æ•°
# total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
# print(f"å¯è®­ç»ƒå‚æ•°æ€»æ•°: {total_params}")

# # è®°å½•æŸå¤±
# train_losses = []
# val_losses = []

# # --------------------------
# # è®­ç»ƒå¾ªç¯
# # --------------------------
# best_val_loss = float('inf')
# patience = 8
# patience_counter = 0

# # åˆ›å»ºç»“æœç›®å½•
# os.makedirs("Transformer/results", exist_ok=True)

# print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
# for epoch in range(1, n_epochs + 1):
#     # è®­ç»ƒé˜¶æ®µ
#     model.train()
#     total_train_loss = 0.0
#     train_batches = 0
    
#     for batch_idx, (X, Y) in enumerate(train_loader):
#         X, Y = X.to(device), Y.to(device)
    
#         # å‰å‘ä¼ æ’­
#         logits = model(src=None, tgt=X)
#         logits_flat = logits.view(-1, vocab_size)
#         Y_flat = Y.view(-1)
    
#         loss = criterion(logits_flat, Y_flat)
    
#         # æ£€æŸ¥NaN
#         if torch.isnan(loss) or torch.isinf(loss):
#             print(f"è·³è¿‡æœ‰é—®é¢˜çš„æ‰¹æ¬¡ {batch_idx}, æŸå¤±: {loss.item()}")
#             continue
            
#         # åå‘ä¼ æ’­
#         optimizer.zero_grad()
#         loss.backward()
        
#         # æ¢¯åº¦è£å‰ª - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
        
#         # æ£€æŸ¥æ¢¯åº¦
#         total_norm = 0.0
#         for p in model.parameters():
#             if p.grad is not None:
#                 param_norm = p.grad.data.norm(2)
#                 total_norm += param_norm.item() ** 2
#         total_norm = total_norm ** 0.5
        
#         if total_norm > 1000:
#             print(f"æ¢¯åº¦çˆ†ç‚¸: {total_norm:.2f}, è·³è¿‡æ›´æ–°")
#             continue
            
#         optimizer.step()
        
#         total_train_loss += loss.item()
#         train_batches += 1
        
#         if batch_idx % 50 == 0:
#             print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}, Grad Norm: {total_norm:.2f}")
    
#     if train_batches == 0:
#         print(f"Epoch {epoch}: æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ‰¹æ¬¡")
#         continue
        
#     avg_train_loss = total_train_loss / train_batches
#     train_losses.append(avg_train_loss)
    
#     # éªŒè¯é˜¶æ®µ
#     model.eval()
#     total_val_loss = 0.0
#     val_batches = 0
    
#     with torch.no_grad():
#         for Xv, Yv in val_loader:
#             Xv, Yv = Xv.to(device), Yv.to(device)
            
#             logits = model(src=None, tgt=Xv)
#             loss = criterion(logits.view(-1, vocab_size), Yv.view(-1))
            
#             if not torch.isnan(loss) and not torch.isinf(loss):
#                 total_val_loss += loss.item()
#                 val_batches += 1
    
#     if val_batches > 0:
#         avg_val_loss = total_val_loss / val_batches
#         val_losses.append(avg_val_loss)
        
#         # æ—©åœæœºåˆ¶
#         if avg_val_loss < best_val_loss:
#             best_val_loss = avg_val_loss
#             torch.save(model.state_dict(), "Transformer/results/best_model.pth")
#             patience_counter = 0
#             print(f"æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
#         else:
#             patience_counter += 1
#             if patience_counter >= patience:
#                 print(f"æ—©åœ: éªŒè¯æŸå¤±åœ¨ {patience} ä¸ªepochå†…æ²¡æœ‰æ”¹å–„")
#                 break
#     else:
#         avg_val_loss = float('inf')
#         val_losses.append(avg_val_loss)
#         print("éªŒè¯é˜¶æ®µæ‰€æœ‰æ‰¹æ¬¡éƒ½å‡ºç°é—®é¢˜")
    
#     # æ›´æ–°å­¦ä¹ ç‡
#     scheduler.step(avg_val_loss)
#     current_lr = optimizer.param_groups[0]['lr']
    
#     print(f"Epoch {epoch:2d}: è®­ç»ƒæŸå¤± = {avg_train_loss:.4f}, éªŒè¯æŸå¤± = {avg_val_loss:.4f}, LR = {current_lr:.2e}")
    
#     # ä¿å­˜æ£€æŸ¥ç‚¹
#     if epoch % 5 == 0:
#         checkpoint_path = f"Transformer/results/checkpoint_epoch{epoch}.pth"
#         torch.save({
#             'epoch': epoch,
#             'model_state_dict': model.state_dict(),
#             'optimizer_state_dict': optimizer.state_dict(),
#             'train_loss': avg_train_loss,
#             'val_loss': avg_val_loss,
#             'random_seed': 42
#         }, checkpoint_path)
#         print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

# # åŠ è½½æœ€ä½³æ¨¡å‹
# if os.path.exists("Transformer/results/best_model.pth"):
#     model.load_state_dict(torch.load("Transformer/results/best_model.pth"))
#     print("åŠ è½½æœ€ä½³æ¨¡å‹å®Œæˆ")

# # æœ€ç»ˆä¿å­˜
# final_model_path = "Transformer/results/transformer_model_final.pth"
# torch.save(model.state_dict(), final_model_path)
# print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path}")

# # ç»˜åˆ¶æŸå¤±æ›²çº¿
# plt.figure(figsize=(10, 6))
# plt.plot(range(1, len(train_losses) + 1), train_losses, 'o-', color='orange', label='Train loss')
# if len(val_losses) == len(train_losses):
#     plt.plot(range(1, len(val_losses) + 1), val_losses, 'o-', color='red', label='Validation loss')
# plt.title('Transformer')
# plt.xlabel('raining rounds')
# plt.ylabel('loss')
# plt.legend()
# plt.grid(True, linestyle='--', alpha=0.7)
# plt.savefig("Transformer/results/loss_curve.png", dpi=300, bbox_inches='tight')
# plt.close()
# print("è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³: Transformer/results/loss_curve.png")

# # æµ‹è¯•æ¨¡å‹ç”Ÿæˆèƒ½åŠ›
# def generate_text(model, start_text, max_length=50, temperature=0.8, top_k=40):
#     """æ”¹è¿›çš„æ–‡æœ¬ç”Ÿæˆå‡½æ•°"""
#     model.eval()
    
#     # å°†èµ·å§‹æ–‡æœ¬åˆ†å‰²æˆå•è¯
#     start_words = start_text.split()
#     tokens = [word_to_idx.get(word, 0) for word in start_words]
    
#     print(f"èµ·å§‹tokens: {tokens}")
#     print(f"èµ·å§‹å•è¯: {start_words}")
    
#     generated_words = start_words.copy()
    
#     with torch.no_grad():
#         for step in range(max_length):
#             # å‡†å¤‡è¾“å…¥ï¼ˆå–æœ€åseq_lenä¸ªtokenï¼‰
#             if len(tokens) > seq_len:
#                 input_tokens = tokens[-seq_len:]
#             else:
#                 input_tokens = tokens
            
#             input_tensor = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0).to(device)
            
#             # å‰å‘ä¼ æ’­
#             logits = model(src=None, tgt=input_tensor)
            
#             # è·å–æœ€åä¸€ä¸ªtokençš„logits
#             next_token_logits = logits[0, -1, :] / max(temperature, 0.1)
            
#             # ä¿®æ­£çš„Top-kè¿‡æ»¤
#             if top_k > 0:
#                 top_k_values, top_k_indices = torch.topk(next_token_logits, min(top_k, len(next_token_logits)))
#                 mask = torch.ones_like(next_token_logits, dtype=torch.bool)
#                 mask[top_k_indices] = False
#                 next_token_logits[mask] = -float('Inf')
            
#             # åº”ç”¨softmaxå¾—åˆ°æ¦‚ç‡
#             probs = torch.softmax(next_token_logits, dim=-1)
            
#             # ä»åˆ†å¸ƒä¸­é‡‡æ ·
#             try:
#                 next_token = torch.multinomial(probs, num_samples=1).item()
#             except:
#                 next_token = torch.argmax(probs).item()
            
#             # å°†tokenè½¬æ¢å›å•è¯
#             next_word = idx_to_word.get(next_token, '<UNK>')  
#             tokens.append(next_token)
#             generated_words.append(next_word)
            
#             # åœæ­¢æ¡ä»¶
#             if next_word in ['.', '!', '?'] and step > 10:
#                 break
#             if next_token == 0:
#                 break
#             if len(generated_words) >= max_length:
#                 break
    
#     generated_text = ' '.join(generated_words)
#     return generated_text
# test_prompts = [
#     "We are accounted poor citizens",
#     "To be or not to be",
#     "Shall I compare thee"
# ]
# for i, prompt in enumerate(test_prompts, 1):
#     print(f"\næµ‹è¯• {i}: '{prompt}'")
#     try:
#         generated = generate_text(model, prompt, max_length=30, temperature=0.8)
#         print(f"ç”Ÿæˆç»“æœ: {generated}")
#         print("-" * 60)
#     except Exception as e:
#         print(f"ç”Ÿæˆå¤±è´¥: {e}")
#         continue

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from model import Transformer
import torch.optim as optim
import requests
import os
import matplotlib.pyplot as plt
import numpy as np
import random
import argparse

# ==================== è§£æå‘½ä»¤è¡Œå‚æ•° ====================
def parse_args():
    parser = argparse.ArgumentParser(description='Transformer è®­ç»ƒè„šæœ¬')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--seq_len', type=int, default=128, help='åºåˆ—é•¿åº¦')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--n_epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-5, help='å­¦ä¹ ç‡')
    parser.add_argument('--dropout', type=float, default=0.1, help='Dropoutç‡')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--d_model', type=int, default=128, help='æ¨¡å‹ç»´åº¦')
    parser.add_argument('--num_heads', type=int, default=8, help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--num_layers', type=int, default=4, help='ç¼–ç å™¨/è§£ç å™¨å±‚æ•°')
    parser.add_argument('--d_ff', type=int, default=512, help='å‰é¦ˆç½‘ç»œç»´åº¦')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--data_path', type=str, default="Transformer/data/tiny_shakespeare.txt", help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--patience', type=int, default=8, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--save_dir', type=str, default="Transformer/results", help='ä¿å­˜ç›®å½•')
    
    return parser.parse_args()

# ==================== éšæœºç§å­è®¾ç½® ====================
def set_seed(seed=42):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    # ç¡®ä¿ç¡®å®šæ€§è®¡ç®—
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"éšæœºç§å­è®¾ç½®ä¸º: {seed}")

# ä¸»å‡½æ•°
def main():
    # è§£æå‚æ•°
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # --------------------------
    # è¶…å‚æ•°è®¾ç½®
    # --------------------------
    seq_len = args.seq_len
    batch_size = args.batch_size
    n_epochs = args.n_epochs
    lr = args.lr
    dropout = args.dropout
    d_model = args.d_model
    num_heads = args.num_heads
    num_layers = args.num_layers
    d_ff = args.d_ff
    pad_idx = 0
    patience = args.patience
    save_dir = args.save_dir

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ‰“å°æ‰€æœ‰å‚æ•°
    print("=" * 50)
    print("è®­ç»ƒå‚æ•°:")
    for arg in vars(args):
        print(f"  {arg}: {getattr(args, arg)}")
    print("=" * 50)

    file_path = args.data_path

    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        print(f"ä»æœ¬åœ°æ–‡ä»¶è¯»å–æˆåŠŸï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")
    else:
        print(f"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        exit(1)

    # åˆ†å‰²æ•°æ®é›†
    split_idx = int(0.9 * len(text))
    train_text = text[:split_idx]
    val_text = text[split_idx:]

    print(f"è®­ç»ƒé›†é•¿åº¦: {len(train_text)} å­—ç¬¦")
    print(f"éªŒè¯é›†é•¿åº¦: {len(val_text)} å­—ç¬¦")

    # åˆ›å»ºå•è¯æ˜ å°„
    words = text.split()
    unique_words = sorted(list(set(words)))
    vocab_size = len(unique_words)
    print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size}")

    # åˆ›å»ºå•è¯åˆ°ç´¢å¼•çš„æ˜ å°„å’Œç´¢å¼•åˆ°å•è¯çš„æ˜ å°„
    word_to_idx = {word: idx for idx, word in enumerate(unique_words)}
    idx_to_word = {idx: word for idx, word in enumerate(unique_words)}
    # ä½¿ç”¨æ­£ç¡®çš„å˜é‡å
    stoi = word_to_idx  
    itos = idx_to_word  

    def create_sequences(text, seq_len, stoi):
        """åˆ›å»ºè®­ç»ƒåºåˆ—"""
        words = text.split()
        data = torch.tensor([word_to_idx.get(word, 0) for word in words], dtype=torch.long)

        # åˆ›å»ºåºåˆ—
        num_sequences = len(data) // seq_len
        total_length = num_sequences * seq_len
        data = data[:total_length]
        
        # é‡å¡‘ä¸ºåºåˆ—
        sequences = data.view(num_sequences, seq_len)
        
        # è¾“å…¥å’Œç›®æ ‡ (teacher forcing)
        inputs = sequences[:, :-1]
        targets = sequences[:, 1:]
        
        return inputs, targets

    print("=== å¤„ç†è®­ç»ƒæ•°æ® ===")
    train_inputs, train_targets = create_sequences(train_text, seq_len, stoi)
    val_inputs, val_targets = create_sequences(val_text, seq_len, stoi)

    print(f"è®­ç»ƒé›†ï¼š{train_inputs.shape} | éªŒè¯é›†ï¼š{val_inputs.shape}")

    # åˆ›å»ºDataLoaderï¼ˆåŒ…å«éšæœºç§å­è®¾ç½®ï¼‰
    def seed_worker(worker_id):
        worker_seed = torch.initial_seed() % 2**32
        np.random.seed(worker_seed)
        random.seed(worker_seed)

    g = torch.Generator()
    g.manual_seed(args.seed)

    train_data = TensorDataset(train_inputs, train_targets)
    val_data = TensorDataset(val_inputs, val_targets)
    train_loader = DataLoader(
        train_data, 
        batch_size=batch_size, 
        shuffle=True, 
        drop_last=True,
        worker_init_fn=seed_worker,
        generator=g
    )
    val_loader = DataLoader(
        val_data, 
        batch_size=batch_size, 
        shuffle=False, 
        drop_last=False
    )

    # --------------------------
    # æ¨¡å‹åˆå§‹åŒ–ä¸ç¨³å®šæ€§æ”¹è¿›
    # --------------------------
    model = Transformer(
        src_vocab_size=vocab_size,
        tgt_vocab_size=vocab_size,
        d_model=d_model,
        num_heads=num_heads,
        num_layers=num_layers,
        d_ff=d_ff,
        dropout=dropout,
        pad_idx=pad_idx
    ).to(device)

    # è‡ªå®šä¹‰æƒé‡åˆå§‹åŒ– - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    def init_weights(m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight, gain=0.1)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Embedding):
            nn.init.normal_(m.weight, mean=0, std=0.01)
    model.apply(init_weights)

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2, betas=(0.9, 0.98), eps=1e-9)

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)

    # å‚æ•°è®¡æ•°
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"å¯è®­ç»ƒå‚æ•°æ€»æ•°: {total_params}")

    # è®°å½•æŸå¤±
    train_losses = []
    val_losses = []

    # --------------------------
    # è®­ç»ƒå¾ªç¯
    # --------------------------
    best_val_loss = float('inf')
    patience_counter = 0

    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(save_dir, exist_ok=True)

    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    for epoch in range(1, n_epochs + 1):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_train_loss = 0.0
        train_batches = 0
        
        for batch_idx, (X, Y) in enumerate(train_loader):
            X, Y = X.to(device), Y.to(device)
        
            # å‰å‘ä¼ æ’­
            logits = model(src=None, tgt=X)
            logits_flat = logits.view(-1, vocab_size)
            Y_flat = Y.view(-1)
        
            loss = criterion(logits_flat, Y_flat)
        
            # æ£€æŸ¥NaN
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"è·³è¿‡æœ‰é—®é¢˜çš„æ‰¹æ¬¡ {batch_idx}, æŸå¤±: {loss.item()}")
                continue
                
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            
            # æ£€æŸ¥æ¢¯åº¦
            total_norm = 0.0
            for p in model.parameters():
                if p.grad is not None:
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
            total_norm = total_norm ** 0.5
            
            if total_norm > 1000:
                print(f"æ¢¯åº¦çˆ†ç‚¸: {total_norm:.2f}, è·³è¿‡æ›´æ–°")
                continue
                
            optimizer.step()
            
            total_train_loss += loss.item()
            train_batches += 1
            
            if batch_idx % 50 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}, Grad Norm: {total_norm:.2f}")
        
        if train_batches == 0:
            print(f"Epoch {epoch}: æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ‰¹æ¬¡")
            continue
            
        avg_train_loss = total_train_loss / train_batches
        train_losses.append(avg_train_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        total_val_loss = 0.0
        val_batches = 0
        
        with torch.no_grad():
            for Xv, Yv in val_loader:
                Xv, Yv = Xv.to(device), Yv.to(device)
                
                logits = model(src=None, tgt=Xv)
                loss = criterion(logits.view(-1, vocab_size), Yv.view(-1))
                
                if not torch.isnan(loss) and not torch.isinf(loss):
                    total_val_loss += loss.item()
                    val_batches += 1
        
        if val_batches > 0:
            avg_val_loss = total_val_loss / val_batches
            val_losses.append(avg_val_loss)
            
            # æ—©åœæœºåˆ¶
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                torch.save(model.state_dict(), f"{save_dir}/best_model.pth")
                patience_counter = 0
                print(f"æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"æ—©åœ: éªŒè¯æŸå¤±åœ¨ {patience} ä¸ªepochå†…æ²¡æœ‰æ”¹å–„")
                    break
        else:
            avg_val_loss = float('inf')
            val_losses.append(avg_val_loss)
            print("éªŒè¯é˜¶æ®µæ‰€æœ‰æ‰¹æ¬¡éƒ½å‡ºç°é—®é¢˜")
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(avg_val_loss)
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f"Epoch {epoch:2d}: è®­ç»ƒæŸå¤± = {avg_train_loss:.4f}, éªŒè¯æŸå¤± = {avg_val_loss:.4f}, LR = {current_lr:.2e}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % 5 == 0:
            checkpoint_path = f"{save_dir}/checkpoint_epoch{epoch}.pth"
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'random_seed': args.seed,
                'args': args
            }, checkpoint_path)
            print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

    # åŠ è½½æœ€ä½³æ¨¡å‹
    if os.path.exists(f"{save_dir}/best_model.pth"):
        model.load_state_dict(torch.load(f"{save_dir}/best_model.pth"))
        print("åŠ è½½æœ€ä½³æ¨¡å‹å®Œæˆ")

    # æœ€ç»ˆä¿å­˜
    final_model_path = f"{save_dir}/transformer_model_final.pth"
    torch.save(model.state_dict(), final_model_path)
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path}")

    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(train_losses) + 1), train_losses, 'o-', color='orange', label='Train loss')
    if len(val_losses) == len(train_losses):
        plt.plot(range(1, len(val_losses) + 1), val_losses, 'o-', color='red', label='Validation loss')
    plt.title('Transformer')
    plt.xlabel('Training rounds')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.savefig(f"{save_dir}/loss_curve.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³: {save_dir}/loss_curve.png")

    # æµ‹è¯•æ¨¡å‹ç”Ÿæˆèƒ½åŠ›
    def generate_text(model, start_text, max_length=50, temperature=0.8, top_k=40):
        """æ”¹è¿›çš„æ–‡æœ¬ç”Ÿæˆå‡½æ•°"""
        model.eval()
        
        # å°†èµ·å§‹æ–‡æœ¬åˆ†å‰²æˆå•è¯
        start_words = start_text.split()
        tokens = [word_to_idx.get(word, 0) for word in start_words]
        
        print(f"èµ·å§‹tokens: {tokens}")
        print(f"èµ·å§‹å•è¯: {start_words}")
        
        generated_words = start_words.copy()
        
        with torch.no_grad():
            for step in range(max_length):
                # å‡†å¤‡è¾“å…¥ï¼ˆå–æœ€åseq_lenä¸ªtokenï¼‰
                if len(tokens) > seq_len:
                    input_tokens = tokens[-seq_len:]
                else:
                    input_tokens = tokens
                
                input_tensor = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0).to(device)
                
                # å‰å‘ä¼ æ’­
                logits = model(src=None, tgt=input_tensor)
                
                # è·å–æœ€åä¸€ä¸ªtokençš„logits
                next_token_logits = logits[0, -1, :] / max(temperature, 0.1)
                
                # ä¿®æ­£çš„Top-kè¿‡æ»¤
                if top_k > 0:
                    top_k_values, top_k_indices = torch.topk(next_token_logits, min(top_k, len(next_token_logits)))
                    mask = torch.ones_like(next_token_logits, dtype=torch.bool)
                    mask[top_k_indices] = False
                    next_token_logits[mask] = -float('Inf')
                
                # åº”ç”¨softmaxå¾—åˆ°æ¦‚ç‡
                probs = torch.softmax(next_token_logits, dim=-1)
                
                # ä»åˆ†å¸ƒä¸­é‡‡æ ·
                try:
                    next_token = torch.multinomial(probs, num_samples=1).item()
                except:
                    next_token = torch.argmax(probs).item()
                
                # å°†tokenè½¬æ¢å›å•è¯
                next_word = idx_to_word.get(next_token, '<UNK>')  
                tokens.append(next_token)
                generated_words.append(next_word)
                
                # åœæ­¢æ¡ä»¶
                if next_word in ['.', '!', '?'] and step > 10:
                    break
                if next_token == 0:
                    break
                if len(generated_words) >= max_length:
                    break
        
        generated_text = ' '.join(generated_words)
        return generated_text

    test_prompts = [
        "We are accounted poor citizens",
        "To be or not to be",
        "Shall I compare thee"
    ]
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\næµ‹è¯• {i}: '{prompt}'")
        try:
            generated = generate_text(model, prompt, max_length=30, temperature=0.8)
            print(f"ç”Ÿæˆç»“æœ: {generated}")
            print("-" * 60)
        except Exception as e:
            print(f"ç”Ÿæˆå¤±è´¥: {e}")
            continue

if __name__ == "__main__":
    main()